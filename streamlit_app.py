# -*- coding: utf-8 -*-
"""
ì‹í’ˆì˜ì–‘ì •ë³´ ë¶„ì„ Streamlit ì›¹ì•±
Original Colab notebook converted to Streamlit interface
"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import os
import seaborn as sns
from io import BytesIO
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì‹í’ˆì˜ì–‘ì •ë³´ ë¶„ì„ê¸°",
    page_title_icon="ğŸ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# í•œê¸€ í°íŠ¸ ì„¤ì • (Streamlit Cloudì—ì„œëŠ” ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©)
plt.rcParams['axes.unicode_minus'] = False

# ìµœì í™”ëœ ë°ì´í„° ë¡œë”© í•¨ìˆ˜ë“¤
@st.cache_data(ttl=3600, show_spinner="ë°ì´í„°ë¥¼ ìµœì í™”í•˜ëŠ” ì¤‘...")  # 1ì‹œê°„ ìºì‹œ
def convert_to_parquet():
    """ì›ë³¸ ë°ì´í„°ë¥¼ Parquet í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥"""
    csv_file_path = './20250327_ê°€ê³µì‹í’ˆDB_147999ê±´.csv'
    xls_file_path = './20250327_ê°€ê³µì‹í’ˆDB_147999ê±´.xlsx'
    parquet_file_path = './nutrition_data_optimized.parquet'
    
    # Parquet íŒŒì¼ì´ ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
    if os.path.exists(parquet_file_path):
        return parquet_file_path
    
    try:
        # ì›ë³¸ ë°ì´í„° ë¡œë“œ
        if os.path.exists(csv_file_path):
            df = pd.read_csv(csv_file_path, encoding='utf-8', low_memory=False)
            st.info(f"CSVì—ì„œ {len(df):,}ê°œ ë ˆì½”ë“œ ë¡œë“œë¨")
        elif os.path.exists(xls_file_path):
            df = pd.read_excel(xls_file_path)
            st.info(f"Excelì—ì„œ {len(df):,}ê°œ ë ˆì½”ë“œ ë¡œë“œë¨")
        else:
            return None
        
        # ë°ì´í„° ì „ì²˜ë¦¬ ë° ìµœì í™”
        columns_to_use = [
            'ì‹í’ˆëª…', 'ëŒ€í‘œì‹í’ˆëª…', 'ì‹í’ˆì†Œë¶„ë¥˜ëª…',
            'ì—ë„ˆì§€(kcal)', 'ë‹¨ë°±ì§ˆ(g)', 'ì§€ë°©(g)', 'íƒ„ìˆ˜í™”ë¬¼(g)', 'ë‹¹ë¥˜(g)',
            'ë‚˜íŠ¸ë¥¨(mg)', 'ì½œë ˆìŠ¤í…Œë¡¤(mg)', 'í¬í™”ì§€ë°©ì‚°(g)', 'íŠ¸ëœìŠ¤ì§€ë°©ì‚°(g)',
            'ì‹ì´ì„¬ìœ (g)', 'ì¹¼ìŠ˜(mg)', 'ì‹í’ˆì¤‘ëŸ‰', 'ì œì¡°ì‚¬ëª…'
        ]
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        existing_cols = [col for col in columns_to_use if col in df.columns]
        df = df[existing_cols]
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.dropna(subset=['ì‹í’ˆëª…'])
        
        # ë°ì´í„° íƒ€ì… ìµœì í™” (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 60-80% ì ˆì•½)
        for col in df.select_dtypes(include=['float64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='float')
            
        for col in df.select_dtypes(include=['int64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='integer')
            
        # ì¹´í…Œê³ ë¦¬í˜• ë°ì´í„° ìµœì í™”
        categorical_cols = ['ì‹í’ˆì†Œë¶„ë¥˜ëª…', 'ì œì¡°ì‚¬ëª…']
        for col in categorical_cols:
            if col in df.columns:
                df[col] = df[col].astype('category')
        
        # Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì••ì¶•ë¥  ë†’ê³  ë¡œë”© ì†ë„ ë¹ ë¦„)
        df.to_parquet(parquet_file_path, compression='snappy', index=False)
        st.success(f"ë°ì´í„°ê°€ Parquet í˜•ì‹ìœ¼ë¡œ ìµœì í™”ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ìš©ëŸ‰: {os.path.getsize(parquet_file_path) / 1024 / 1024:.1f}MB)")
        
        return parquet_file_path
        
    except Exception as e:
        st.error(f"ë°ì´í„° ë³€í™˜ ì˜¤ë¥˜: {e}")
        return None

@st.cache_data(ttl=3600, show_spinner="ê³ ì„±ëŠ¥ ë°ì´í„° ë¡œë”© ì¤‘...")  # 1ì‹œê°„ ìºì‹œ
def load_optimized_data():
    """ìµœì í™”ëœ Parquet íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (10-50ë°° ë¹ ë¦„)"""
    parquet_file_path = './nutrition_data_optimized.parquet'
    
    # Parquet íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(parquet_file_path):
        parquet_path = convert_to_parquet()
        if parquet_path is None:
            return None
    
    try:
        # Parquetì—ì„œ ì´ˆê³ ì† ë¡œë“œ (ì¼ë°˜ì ìœ¼ë¡œ CSV ëŒ€ë¹„ 5-10ë°° ë¹ ë¦„)
        df = pd.read_parquet(parquet_file_path)
        return df
        
    except Exception as e:
        st.error(f"ìµœì í™”ëœ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
        return None

@st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ
def load_data_from_sqlite():
    """SQLite ë¡œì»¬ DBì—ì„œ ì´ˆê³ ì† ë¡œë“œ"""
    db_path = './nutrition_data.db'
    
    if not os.path.exists(db_path):
        # DBê°€ ì—†ìœ¼ë©´ ìƒì„±
        df_temp = load_optimized_data()
        if df_temp is not None:
            create_sqlite_db(df_temp, db_path)
        else:
            return None
    
    try:
        # SQLiteì—ì„œ ì´ˆê³ ì† ë¡œë“œ
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT * FROM nutrition_data", conn)
        conn.close()
        return df
    except Exception as e:
        st.error(f"SQLite ë¡œë”© ì˜¤ë¥˜: {e}")
        return None

def create_sqlite_db(df, db_path):
    """DataFrameì„ SQLite DBë¡œ ì €ì¥"""
    try:
        conn = sqlite3.connect(db_path)
        df.to_sql('nutrition_data', conn, if_exists='replace', index=False)
        
        # ê²€ìƒ‰ ì„±ëŠ¥ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
        cursor = conn.cursor()
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_food_name ON nutrition_data(ì‹í’ˆëª…)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_category ON nutrition_data(ì‹í’ˆì†Œë¶„ë¥˜ëª…)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_manufacturer ON nutrition_data(ì œì¡°ì‚¬ëª…)")
        
        conn.commit()
        conn.close()
        st.success(f"SQLite DB ìƒì„± ì™„ë£Œ: {db_path}")
    except Exception as e:
        st.error(f"SQLite DB ìƒì„± ì˜¤ë¥˜: {e}")

@st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ  
def load_data_from_pickle():
    """Pickle íŒŒì¼ì—ì„œ ì´ˆê³ ì† ë¡œë“œ (Python ê°ì²´ ì§ë ¬í™”)"""
    pickle_path = './nutrition_data.pkl'
    
    if not os.path.exists(pickle_path):
        # Pickle íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
        df_temp = load_optimized_data()
        if df_temp is not None:
            with open(pickle_path, 'wb') as f:
                pickle.dump(df_temp, f, protocol=pickle.HIGHEST_PROTOCOL)
            st.success(f"Pickle ìºì‹œ ìƒì„±: {pickle_path}")
        else:
            return None
    
    try:
        # Pickleì—ì„œ ì´ˆê³ ì† ë¡œë“œ (ì¢…ì¢… ê°€ì¥ ë¹ ë¦„)
        with open(pickle_path, 'rb') as f:
            df = pickle.load(f)
        return df
    except Exception as e:
        st.error(f"Pickle ë¡œë”© ì˜¤ë¥˜: {e}")
        return None

# PostgreSQL/MySQL ì—°ë™ (ì„ íƒì‚¬í•­)
@st.cache_data(ttl=900)  # 15ë¶„ ìºì‹œ
def load_data_from_database():
    """PostgreSQL/MySQLì—ì„œ ë°ì´í„° ë¡œë“œ (í”„ë¡œë•ì…˜ í™˜ê²½)"""
    
    # Streamlit secretsì—ì„œ DB ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    # secrets.toml íŒŒì¼ì— ì„¤ì •:
    # [database]
    # host = "your-host"
    # port = 5432
    # database = "nutrition_db"
    # username = "your-user"
    # password = "your-password"
    
    try:
        # PostgreSQL ì—°ê²° ì˜ˆì‹œ
        db_config = st.secrets.get("database", {})
        if not db_config:
            return None
            
        engine = create_engine(
            f"postgresql://{db_config['username']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['database']}"
        )
        
        # ì¿¼ë¦¬ ìµœì í™”: í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        query = """
        SELECT ì‹í’ˆëª…, ëŒ€í‘œì‹í’ˆëª…, ì‹í’ˆì†Œë¶„ë¥˜ëª…, ì—ë„ˆì§€_kcal as "ì—ë„ˆì§€(kcal)",
               ë‹¨ë°±ì§ˆ_g as "ë‹¨ë°±ì§ˆ(g)", ì§€ë°©_g as "ì§€ë°©(g)", íƒ„ìˆ˜í™”ë¬¼_g as "íƒ„ìˆ˜í™”ë¬¼(g)",
               ë‹¹ë¥˜_g as "ë‹¹ë¥˜(g)", ë‚˜íŠ¸ë¥¨_mg as "ë‚˜íŠ¸ë¥¨(mg)", ì½œë ˆìŠ¤í…Œë¡¤_mg as "ì½œë ˆìŠ¤í…Œë¡¤(mg)",
               í¬í™”ì§€ë°©ì‚°_g as "í¬í™”ì§€ë°©ì‚°(g)", ì‹ì´ì„¬ìœ _g as "ì‹ì´ì„¬ìœ (g)", 
               ì¹¼ìŠ˜_mg as "ì¹¼ìŠ˜(mg)", ì‹í’ˆì¤‘ëŸ‰, ì œì¡°ì‚¬ëª…
        FROM nutrition_data 
        ORDER BY ì‹í’ˆëª…
        """
        
        df = pd.read_sql(query, engine)
        engine.dispose()
        return df
        
    except Exception as e:
        # DB ì—°ê²° ì‹¤íŒ¨ì‹œ ì¡°ìš©íˆ ë„˜ì–´ê° (ë°±ì—… ë¡œë”© ë°©ë²• ì‚¬ìš©)
        return None

def load_data():
    """ì´ˆê³ ì† ë°ì´í„° ë¡œë”© - ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìµœì  ê²½ë¡œ ìë™ ì„ íƒ"""
    
    import time
    start_time = time.time()
    
    # 1ìˆœìœ„: Pickle ìºì‹œ (ê°€ì¥ ë¹ ë¦„)
    df = load_data_from_pickle()
    if df is not None:
        load_time = time.time() - start_time
        st.sidebar.success(f"ğŸš€ Pickle ìºì‹œ: {len(df):,}ê°œ ì‹í’ˆ ({load_time:.2f}ì´ˆ)")
        return df
    
    # 2ìˆœìœ„: SQLite ë¡œì»¬ DB
    df = load_data_from_sqlite()
    if df is not None:
        load_time = time.time() - start_time
        st.sidebar.success(f"ğŸ’¾ SQLite DB: {len(df):,}ê°œ ì‹í’ˆ ({load_time:.2f}ì´ˆ)")
        return df
    
    # 3ìˆœìœ„: ìµœì í™”ëœ Parquet íŒŒì¼
    df = load_optimized_data()
    if df is not None:
        load_time = time.time() - start_time
        st.sidebar.success(f"âš¡ Parquet: {len(df):,}ê°œ ì‹í’ˆ ({load_time:.2f}ì´ˆ)")
        return df
    
    # 4ìˆœìœ„: ì›ê²© ë°ì´í„°ë² ì´ìŠ¤
    df = load_data_from_database()
    if df is not None:
        load_time = time.time() - start_time
        st.sidebar.info(f"ğŸŒ ì›ê²© DB: {len(df):,}ê°œ ì‹í’ˆ ({load_time:.2f}ì´ˆ)")
        return df
    
    # 5ìˆœìœ„: Google Driveì—ì„œ ë‹¤ìš´ë¡œë“œ https://docs.google.com/spreadsheets/d/1FrAR9SRDVbppLbeP-F2IFY3FQwLWB1oX/edit
    if GOOGLE_DRIVE_CONFIG["file_id"] != "1FrAR9SRDVbppLbeP-F2IFY3FQwLWB1oX":
        google_drive_file = download_from_google_drive(
            GOOGLE_DRIVE_CONFIG["file_id"], 
            GOOGLE_DRIVE_CONFIG["file_name"]
        )
        if google_drive_file and os.path.exists(google_drive_file):
            try:
                df = pd.read_excel(google_drive_file)
                load_time = time.time() - start_time
                st.sidebar.info(f"â˜ï¸ Google Drive: {len(df):,}ê°œ ì‹í’ˆ ({load_time:.2f}ì´ˆ)")
                
                # ì „ì²˜ë¦¬ ë° ìºì‹œ ìƒì„±
                df = preprocess_dataframe(df)
                create_all_caches(df)
                return df
            except Exception as e:
                st.error(f"Google Drive íŒŒì¼ ë¡œë”© ì˜¤ë¥˜: {e}")
    
    # 6ìˆœìœ„: ë¡œì»¬ íŒŒì¼ ê²€ìƒ‰
    local_files = ['./20250327_ê°€ê³µì‹í’ˆDB_147999ê±´.csv', './20250327_ê°€ê³µì‹í’ˆDB_147999ê±´.xlsx']
    
    for file_path in local_files:
        if os.path.exists(file_path):
            try:
                if file_path.endswith('.csv'):
                    df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)
                    st.sidebar.warning(f"ğŸ“ ë¡œì»¬ CSV: {len(df):,}ê°œ ì‹í’ˆ")
                else:
                    df = pd.read_excel(file_path)
                    st.sidebar.warning(f"ğŸ“ ë¡œì»¬ Excel: {len(df):,}ê°œ ì‹í’ˆ")
                
                load_time = time.time() - start_time
                st.sidebar.info(f"ë¡œë”© ì‹œê°„: {load_time:.2f}ì´ˆ")
                
                # ì „ì²˜ë¦¬ ë° ìºì‹œ ìƒì„±
                df = preprocess_dataframe(df)
                create_all_caches(df)
                return df
                
            except Exception as e:
                st.error(f"{file_path} ë¡œë”© ì˜¤ë¥˜: {e}")
                continue
    
    # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ì‹œ íŒŒì¼ ì—…ë¡œë“œ ìœ ë„
    return None

def preprocess_dataframe(df):
    """DataFrame ì „ì²˜ë¦¬"""
    columns_to_use = [
        'ì‹í’ˆëª…', 'ëŒ€í‘œì‹í’ˆëª…', 'ì‹í’ˆì†Œë¶„ë¥˜ëª…',
        'ì—ë„ˆì§€(kcal)', 'ë‹¨ë°±ì§ˆ(g)', 'ì§€ë°©(g)', 'íƒ„ìˆ˜í™”ë¬¼(g)', 'ë‹¹ë¥˜(g)',
        'ë‚˜íŠ¸ë¥¨(mg)', 'ì½œë ˆìŠ¤í…Œë¡¤(mg)', 'í¬í™”ì§€ë°©ì‚°(g)', 'íŠ¸ëœìŠ¤ì§€ë°©ì‚°(g)',
        'ì‹ì´ì„¬ìœ (g)', 'ì¹¼ìŠ˜(mg)', 'ì‹í’ˆì¤‘ëŸ‰', 'ì œì¡°ì‚¬ëª…'
    ]
    
    existing_cols = [col for col in columns_to_use if col in df.columns]
    df = df[existing_cols]
    df = df.dropna(subset=['ì‹í’ˆëª…'])
    
    # ë°ì´í„° íƒ€ì… ìµœì í™”
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='float')
    
    categorical_cols = ['ì‹í’ˆì†Œë¶„ë¥˜ëª…', 'ì œì¡°ì‚¬ëª…']
    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].astype('category')
    
    return df

def create_all_caches(df):
    """ëª¨ë“  ìºì‹œ í˜•íƒœ ìƒì„±"""
    with st.spinner("âš¡ ê³ ì† ìºì‹œ ìƒì„± ì¤‘... (ë‹¤ìŒë²ˆë¶€í„° ì´ˆê³ ì† ë¡œë”©!)"):
        # Parquet íŒŒì¼
        parquet_path = './nutrition_data_optimized.parquet'
        df.to_parquet(parquet_path, compression='snappy', index=False)
        
        # Pickle ìºì‹œ
        pickle_path = './nutrition_data.pkl'
        with open(pickle_path, 'wb') as f:
            pickle.dump(df, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        # SQLite DB
        create_sqlite_db(df, './nutrition_data.db')
        
    st.success("ğŸ‰ ìºì‹œ ìƒì„± ì™„ë£Œ! ë‹¤ìŒë²ˆë¶€í„°ëŠ” ì´ˆê³ ì† ë¡œë”©ë©ë‹ˆë‹¤.")

# ì¼ì¼ ê¶Œì¥ëŸ‰ ê¸°ì¤€ ì„¤ì •
DAILY_RECOMMENDATIONS = {
    'ì—ë„ˆì§€(kcal)': 2400,
    'ë‹¨ë°±ì§ˆ(g)': 65,
    'ì§€ë°©(g)': 65,
    'íƒ„ìˆ˜í™”ë¬¼(g)': 320,
    'ë‹¹ë¥˜(g)': 50,
    'ë‚˜íŠ¸ë¥¨(mg)': 2000,
    'ì¹¼ìŠ˜(mg)': 700,
    'ì½œë ˆìŠ¤í…Œë¡¤(mg)': 300,
    'í¬í™”ì§€ë°©ì‚°(g)': 15,
    'ì‹ì´ì„¬ìœ (g)': 25,
}

@st.cache_data
def expand_table(df):
    """ì¼ì¼ ê¶Œì¥ëŸ‰ ëŒ€ë¹„ ë¹„ìœ¨ ê³„ì‚°"""
    df_expanded = df.copy()
    for nutrient, recommended in DAILY_RECOMMENDATIONS.items():
        if nutrient in df_expanded.columns:
            new_col_name = nutrient.replace('(g)', '(%)').replace('(kcal)', '(%)').replace('(mg)', '(%)')
            df_expanded[new_col_name] = (df_expanded[nutrient] / recommended * 100).round(1)
    return df_expanded

def evaluate_nutri_score(row):
    """Nutri-Score ê¸°ë°˜ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°"""
    # Bad ìš”ì†Œ ì ìˆ˜ í•¨ìˆ˜ë“¤
    def score_energy(kcal):
        thresholds = [335, 670, 1005, 1340, 1675, 2010, 2345, 2680, 3015, 3350]
        return sum(kcal > t for t in thresholds)
    
    def score_sugars(g):
        thresholds = [4.5, 9, 13.5, 18, 22.5, 27, 31, 36, 40, 45]
        return sum(g > t for t in thresholds)
    
    def score_saturated_fat(g):
        thresholds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
        return sum(g > t for t in thresholds)
    
    def score_sodium(mg):
        thresholds = [90, 180, 270, 360, 450, 540, 630, 720, 810, 900]
        return sum(mg > t for t in thresholds)
    
    # Good ìš”ì†Œ ì ìˆ˜ í•¨ìˆ˜ë“¤
    def score_fiber(g):
        thresholds = [0.9, 1.9, 2.8, 3.7, 4.7]
        return sum(g > t for t in thresholds)
    
    def score_protein(g):
        thresholds = [1.6, 3.2, 4.8, 6.4, 8.0]
        return sum(g > t for t in thresholds)
    
    # ìˆ˜ì¹˜ ê°€ì ¸ì˜¤ê¸°
    kcal = row.get("ì—ë„ˆì§€(kcal)", 0)
    sugars = row.get("ë‹¹ë¥˜(g)", 0)
    sat_fat = row.get("í¬í™”ì§€ë°©ì‚°(g)", 0)
    sodium = row.get("ë‚˜íŠ¸ë¥¨(mg)", 0)
    fiber = row.get("ì‹ì´ì„¬ìœ (g)", 0)
    protein = row.get("ë‹¨ë°±ì§ˆ(g)", 0)
    
    # ì ìˆ˜ ê³„ì‚°
    bad = score_energy(kcal) + score_sugars(sugars) + score_saturated_fat(sat_fat) + score_sodium(sodium)
    good = score_fiber(fiber) + score_protein(protein)
    nutri_score = bad - good
    
    # ë“±ê¸‰ ë³€í™˜
    if nutri_score <= -1:
        grade = "A"
    elif nutri_score <= 2:
        grade = "B"
    elif nutri_score <= 10:
        grade = "C"
    elif nutri_score <= 18:
        grade = "D"
    else:
        grade = "E"
    
    return pd.Series({
        "ê±´ê°•ë„ì ìˆ˜": (40 - nutri_score),
        "ê±´ê°•ë“±ê¸‰": grade
    })

def search_foods(df, keyword):
    """í‚¤ì›Œë“œë¡œ ì‹í’ˆ ê²€ìƒ‰"""
    if df is None or not keyword:
        return df
    
    keywords = keyword.strip().split()
    condition = df['ì‹í’ˆëª…'].str.contains(keywords[0], case=False, na=False) | \
                df['ëŒ€í‘œì‹í’ˆëª…'].str.contains(keywords[0], case=False, na=False) | \
                df['ì‹í’ˆì†Œë¶„ë¥˜ëª…'].str.contains(keywords[0], case=False, na=False)
    
    for kw in keywords[1:]:
        cond = df['ì‹í’ˆëª…'].str.contains(kw, case=False, na=False) | \
               df['ëŒ€í‘œì‹í’ˆëª…'].str.contains(kw, case=False, na=False) | \
               df['ì‹í’ˆì†Œë¶„ë¥˜ëª…'].str.contains(kw, case=False, na=False)
        condition = condition & cond
    
    return df[condition]

def create_plotly_boxplot(df):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ë°•ìŠ¤í”Œë¡¯ ìƒì„±"""
    if df is None or len(df) == 0:
        return None
        
    # ìƒìœ„ 15ê°œ ë¶„ë¥˜
    top15_classes = df.groupby('ì‹í’ˆì†Œë¶„ë¥˜ëª…')['ê±´ê°•ë„ì ìˆ˜'].mean().nlargest(15).index
    filtered_df = df[df['ì‹í’ˆì†Œë¶„ë¥˜ëª…'].isin(top15_classes)]
    
    fig = px.box(filtered_df, 
                 x='ì‹í’ˆì†Œë¶„ë¥˜ëª…', 
                 y='ê±´ê°•ë„ì ìˆ˜',
                 title='ê±´ê°•ë„ ì ìˆ˜ í‰ê·  ìƒìœ„ 15ê°œ (ì‹í’ˆì†Œë¶„ë¥˜ë³„ ë¶„í¬)',
                 color='ì‹í’ˆì†Œë¶„ë¥˜ëª…')
    
    fig.update_layout(
        xaxis_tickangle=-45,
        height=500,
        showlegend=False,
        xaxis_title="ì‹í’ˆì†Œë¶„ë¥˜",
        yaxis_title="ê±´ê°•ë„ì ìˆ˜"
    )
    
    return fig

def create_nutrition_chart_plotly(search_result, keyword, top_n=10):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ì˜ì–‘ì„±ë¶„ ë¹„êµ ì°¨íŠ¸"""
    if search_result is None or len(search_result) == 0:
        return None, None
    
    top_products = search_result.head(top_n)
    elements = ["ì—ë„ˆì§€(%)", "ë‹¹ë¥˜(%)", "í¬í™”ì§€ë°©ì‚°(%)", 'ë‚˜íŠ¸ë¥¨(%)', 'ë‹¨ë°±ì§ˆ(%)', "ì‹ì´ì„¬ìœ (%)"]
    existing_elements = [n for n in elements if n in top_products.columns]
    
    fig = go.Figure()
    
    for element in existing_elements:
        fig.add_trace(go.Bar(
            name=element,
            x=top_products['ì‹í’ˆëª…'].str[:20],  # ê¸´ ì´ë¦„ ìë¥´ê¸°
            y=top_products[element],
            opacity=0.8
        ))
    
    fig.update_layout(
        title=f'"{keyword}" ê²€ìƒ‰ ê²°ê³¼ - ì˜ì–‘ì„±ë¶„ ë¶„ì„ (ìƒìœ„ {top_n}ê°œ)',
        xaxis_title='ì‹í’ˆëª…',
        yaxis_title='í•¨ëŸ‰(%)',
        barmode='group',
        height=500,
        xaxis_tickangle=-45
    )
    
    return fig, top_products

def create_correlation_heatmap_plotly(df_clean, existing_cols):
    """Plotlyë¥¼ ì‚¬ìš©í•œ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ"""
    if df_clean is None or len(df_clean) == 0:
        return None, None
        
    correlation_matrix = df_clean[existing_cols].corr()
    
    fig = px.imshow(correlation_matrix, 
                    text_auto=True, 
                    aspect="auto",
                    title='ì‹í’ˆ ì˜ì–‘ì„±ë¶„ ê°„ ìƒê´€ê´€ê³„',
                    color_continuous_scale='RdBu_r',
                    color_continuous_midpoint=0)
    
    fig.update_layout(height=500)
    return fig, correlation_matrix

# ë©”ì¸ ì•±
def main():
    st.title("ğŸ ì‹í’ˆì˜ì–‘ì •ë³´ ë¶„ì„ê¸°")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°”
    st.sidebar.title("ğŸ“‹ ë¶„ì„ ì˜µì…˜")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • (ì„ íƒì‚¬í•­)
    with st.sidebar.expander("âš™ï¸ ì„±ëŠ¥ ì„¤ì •"):
        if st.button("ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”"):
            # ëª¨ë“  ìºì‹œ íŒŒì¼ ì‚­ì œ
            cache_files = [
                './nutrition_data_optimized.parquet',
                './nutrition_data.pkl', 
                './nutrition_data.db'
            ]
            for cache_file in cache_files:
                if os.path.exists(cache_file):
                    os.remove(cache_file)
            st.cache_data.clear()
            st.success("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.experimental_rerun()
        
        st.info("ğŸ’¡ ìµœì´ˆ ì‹¤í–‰ í›„ ë¡œë”© ì†ë„ê°€ 10-50ë°° í–¥ìƒë©ë‹ˆë‹¤")
    
    # ê³ ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ
    df = load_data()
    
    if df is None:
        st.error("ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # íŒŒì¼ ì—…ë¡œë“œ ì˜µì…˜ ì œê³µ
        uploaded_file = st.file_uploader(
            "CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", 
            type=['csv', 'xlsx']
        )
        if uploaded_file:
            try:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file, encoding='utf-8')
                else:
                    df = pd.read_excel(uploaded_file)
                st.success("íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
            except Exception as e:
                st.error(f"íŒŒì¼ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
                return
        else:
            return
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    df = expand_table(df)
    df[['ê±´ê°•ë„ì ìˆ˜', 'ê±´ê°•ë“±ê¸‰']] = df.apply(evaluate_nutri_score, axis=1)
    
    # ê¸°ë³¸ ì •ë³´ í‘œì‹œ
    st.sidebar.markdown(f"**ğŸ“Š ì´ ì‹í’ˆ ìˆ˜:** {len(df):,}ê°œ")
    st.sidebar.markdown(f"**ğŸ·ï¸ ì‹í’ˆ ë¶„ë¥˜:** {df['ì‹í’ˆì†Œë¶„ë¥˜ëª…'].nunique()}ê°œ")
    
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì…ë ¥
    keyword = st.sidebar.text_input("ğŸ” ì‹í’ˆ ê²€ìƒ‰", placeholder="ì˜ˆ: ì•„ì´ìŠ¤í¬ë¦¼, ë¼ë©´")
    
    # ê²€ìƒ‰ ê²°ê³¼
    if keyword:
        result = search_foods(df, keyword)
        st.sidebar.markdown(f"**ê²€ìƒ‰ ê²°ê³¼:** {len(result)}ê°œ ì‹í’ˆ")
    else:
        result = df
        keyword = "ì „ì²´ ì‹í’ˆ"
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if len(result) > 0:
        # íƒ­ êµ¬ì„±
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ë¶„ë¥˜ë³„ ë¶„ì„", "ğŸ¥‡ ìƒìœ„ ì œí’ˆ", "ğŸ“ˆ ìƒê´€ê´€ê³„ ë¶„ì„", "ğŸ“‹ ìƒì„¸ ë°ì´í„°"])
        
        with tab1:
            st.subheader(f"'{keyword}' - ì‹í’ˆì†Œë¶„ë¥˜ë³„ ê±´ê°•ë„ ì ìˆ˜ ë¶„í¬")
            fig_box = create_plotly_boxplot(result)
            if fig_box:
                st.plotly_chart(fig_box, use_container_width=True)
            else:
                st.info("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with tab2:
            st.subheader(f"'{keyword}' - ê±´ê°•ë„ ì ìˆ˜ ìƒìœ„ ì œí’ˆ")
            
            # ìƒìœ„ ì œí’ˆ ìˆ˜ ì„ íƒ
            top_n = st.selectbox("í‘œì‹œí•  ì œí’ˆ ìˆ˜", [5, 10, 15, 20], index=1)
            
            # ê±´ê°•ë„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            result_sorted = result.sort_values(by='ê±´ê°•ë„ì ìˆ˜', ascending=False)
            
            fig_nutrition, top_products = create_nutrition_chart_plotly(result_sorted, keyword, top_n)
            if fig_nutrition:
                st.plotly_chart(fig_nutrition, use_container_width=True)
                
                # ìƒìœ„ ì œí’ˆ í…Œì´ë¸”
                st.subheader("ìƒìœ„ ì œí’ˆ ìƒì„¸ ì •ë³´")
                display_cols = ['ì‹í’ˆëª…', 'ì‹í’ˆì†Œë¶„ë¥˜ëª…', 'ì œì¡°ì‚¬ëª…', 'ê±´ê°•ë„ì ìˆ˜', 'ê±´ê°•ë“±ê¸‰']
                existing_display_cols = [col for col in display_cols if col in top_products.columns]
                st.dataframe(top_products[existing_display_cols], use_container_width=True)
            else:
                st.info("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with tab3:
            st.subheader(f"'{keyword}' - ì˜ì–‘ì„±ë¶„ ìƒê´€ê´€ê³„ ë¶„ì„")
            
            # ë¶„ì„í•  ì˜ì–‘ì„±ë¶„ ì„ íƒ
            available_nutrients = ['ë‹¨ë°±ì§ˆ(%)', 'ì§€ë°©(%)', 'íƒ„ìˆ˜í™”ë¬¼(%)', 'ë‹¹ë¥˜(%)', 'ë‚˜íŠ¸ë¥¨(%)', 'ì½œë ˆìŠ¤í…Œë¡¤(%)']
            existing_nutrients = [n for n in available_nutrients if n in result.columns]
            
            if existing_nutrients:
                selected_nutrients = st.multiselect(
                    "ë¶„ì„í•  ì˜ì–‘ì„±ë¶„ ì„ íƒ",
                    existing_nutrients,
                    default=existing_nutrients[:4]
                )
                
                if len(selected_nutrients) >= 2:
                    df_clean = result[selected_nutrients].dropna()
                    
                    if len(df_clean) > 1:
                        fig_corr, corr_matrix = create_correlation_heatmap_plotly(df_clean, selected_nutrients)
                        if fig_corr:
                            st.plotly_chart(fig_corr, use_container_width=True)
                            
                            # ìƒê´€ê³„ìˆ˜ í…Œì´ë¸”
                            st.subheader("ìƒê´€ê³„ìˆ˜ ë§¤íŠ¸ë¦­ìŠ¤")
                            st.dataframe(corr_matrix.round(3), use_container_width=True)
                        else:
                            st.info("ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                    else:
                        st.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                else:
                    st.info("ìµœì†Œ 2ê°œ ì´ìƒì˜ ì˜ì–‘ì„±ë¶„ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                st.info("ë¶„ì„ ê°€ëŠ¥í•œ ì˜ì–‘ì„±ë¶„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with tab4:
            st.subheader(f"'{keyword}' - ìƒì„¸ ë°ì´í„° ({len(result)}ê°œ)")
            
            # ì»¬ëŸ¼ ì„ íƒ
            all_columns = result.columns.tolist()
            default_columns = [
                'ì‹í’ˆëª…', 'ëŒ€í‘œì‹í’ˆëª…', 'ì‹í’ˆì†Œë¶„ë¥˜ëª…', 'ì œì¡°ì‚¬ëª…',
                'ì—ë„ˆì§€(%)', 'ë‹¨ë°±ì§ˆ(%)', 'ì§€ë°©(%)', 'íƒ„ìˆ˜í™”ë¬¼(%)',
                'ê±´ê°•ë„ì ìˆ˜', 'ê±´ê°•ë“±ê¸‰'
            ]
            existing_default_cols = [col for col in default_columns if col in all_columns]
            
            selected_columns = st.multiselect(
                "í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ",
                all_columns,
                default=existing_default_cols
            )
            
            if selected_columns:
                # ë°ì´í„° í•„í„°ë§ ì˜µì…˜
                col1, col2 = st.columns(2)
                
                with col1:
                    if 'ê±´ê°•ë“±ê¸‰' in result.columns:
                        grade_filter = st.multiselect(
                            "ê±´ê°•ë“±ê¸‰ í•„í„°",
                            result['ê±´ê°•ë“±ê¸‰'].unique(),
                            default=result['ê±´ê°•ë“±ê¸‰'].unique()
                        )
                        result = result[result['ê±´ê°•ë“±ê¸‰'].isin(grade_filter)]
                
                with col2:
                    if 'ì‹í’ˆì†Œë¶„ë¥˜ëª…' in result.columns:
                        category_options = result['ì‹í’ˆì†Œë¶„ë¥˜ëª…'].unique()
                        if len(category_options) > 1:
                            category_filter = st.multiselect(
                                "ì‹í’ˆë¶„ë¥˜ í•„í„°",
                                category_options,
                                default=category_options
                            )
                            result = result[result['ì‹í’ˆì†Œë¶„ë¥˜ëª…'].isin(category_filter)]
                
                # ë°ì´í„° í‘œì‹œ
                st.dataframe(
                    result[selected_columns].fillna('-'),
                    use_container_width=True,
                    height=400
                )
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                csv = result[selected_columns].to_csv(index=False, encoding='utf-8')
                st.download_button(
                    label="ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name=f"{keyword}_ë¶„ì„ê²°ê³¼.csv",
                    mime="text/csv"
                )
            else:
                st.info("í‘œì‹œí•  ì»¬ëŸ¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
    
    # í‘¸í„°
    st.markdown("---")
    st.markdown("ğŸ”¬ **ì˜ì–‘ì„±ë¶„ ë¶„ì„ ê¸°ì¤€:** Nutri-Score ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°")
    st.markdown("ğŸ“Š **ì¼ì¼ ê¶Œì¥ëŸ‰:** ì„±ì¸ ë‚¨ì„± 50ì„¸ ê¸°ì¤€")

if __name__ == "__main__":
    main()
